"""
Train classifier for scam detection
"""
import os
import json
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
from transformers import (
    AutoTokenizer, 
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer,
    DataCollatorWithPadding
)
from datasets import Dataset
import torch
from typing import Dict, List, Tuple
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class ScamClassifierTrainer:
    """Trainer for scam detection classifier"""
    
    def __init__(self, model_name: str = "xlm-roberta-base"):
        self.model_name = model_name
        self.tokenizer = None
        self.model = None
        self.training_args = None
        
    def load_data(self, data_path: str) -> Tuple[List[str], List[int]]:
        """Load training data"""
        logger.info(f"Loading data from {data_path}")
        
        # Load sample data (in production, load from actual dataset)
        texts = []
        labels = []
        
        # Sample scam texts
        scam_texts = [
            "Send â‚¹1000 to UPI abc@upi to claim your lottery prize of â‚¹50,000! Limited time offer.",
            "Urgent: Your account will be closed. Send money immediately to avoid penalty.",
            "Congratulations! You won â‚¹1,00,000. Click here to claim your prize now!",
            "Act now! Limited time offer. Send â‚¹500 to get â‚¹50,000 instantly.",
            "Your bank account is suspended. Send verification fee to restore access.",
            "à¤¤à¤¤à¥à¤•à¤¾à¤² â‚¹1000 UPI abc@upi à¤ªà¤° à¤­à¥‡à¤œà¥‡à¤‚ à¤”à¤° â‚¹50,000 à¤•à¤¾ à¤²à¥‰à¤Ÿà¤°à¥€ à¤ªà¥à¤°à¤¸à¥à¤•à¤¾à¤° à¤œà¥€à¤¤à¥‡à¤‚!",
            "à¤…à¤µà¤¸à¤° à¤–à¥‹à¤¨à¤¾ à¤®à¤¤! à¤¸à¥€à¤®à¤¿à¤¤ à¤¸à¤®à¤¯ à¤•à¤¾ à¤ªà¥à¤°à¤¸à¥à¤¤à¤¾à¤µà¥¤ à¤…à¤­à¥€ à¤ªà¥ˆà¤¸à¥‡ à¤­à¥‡à¤œà¥‡à¤‚à¥¤",
            "â‚¹1000 à® UPI abc@upi à®•à¯à®•à¯ à®‰à®Ÿà®©à®Ÿà®¿à®¯à®¾à®• à®…à®©à¯à®ªà¯à®ªà®¿ â‚¹50,000 à®²à®¾à®Ÿà¯à®Ÿà®°à®¿ à®ªà®°à®¿à®šà¯ˆ à®µà¯†à®²à¯à®²à¯à®™à¯à®•à®³à¯!",
            "à²¤à²•à³à²·à²£ â‚¹1000 à²…à²¨à³à²¨à³ UPI abc@upi à²—à³† à²•à²³à³à²¹à²¿à²¸à²¿ â‚¹50,000 à²²à²¾à²Ÿà²°à²¿ à²¬à²¹à³à²®à²¾à²¨à²µà²¨à³à²¨à³ à²—à³†à²²à³à²²à²¿!"
        ]
        
        # Sample legitimate texts
        legitimate_texts = [
            "The Earth is round and orbits around the Sun.",
            "Water boils at 100 degrees Celsius at sea level.",
            "COVID-19 vaccines are safe and effective.",
            "Regular exercise is important for good health.",
            "The capital of India is New Delhi.",
            "à¤ªà¥ƒà¤¥à¥à¤µà¥€ à¤—à¥‹à¤² à¤¹à¥ˆ à¤”à¤° à¤¸à¥‚à¤°à¥à¤¯ à¤•à¥‡ à¤šà¤¾à¤°à¥‹à¤‚ à¤“à¤° à¤˜à¥‚à¤®à¤¤à¥€ à¤¹à¥ˆà¥¤",
            "à¤œà¤² à¤¸à¤®à¥à¤¦à¥à¤° à¤¤à¤² à¤ªà¤° 100 à¤¡à¤¿à¤—à¥à¤°à¥€ à¤¸à¥‡à¤²à¥à¤¸à¤¿à¤¯à¤¸ à¤ªà¤° à¤‰à¤¬à¤²à¤¤à¤¾ à¤¹à¥ˆà¥¤",
            "à®ªà¯‚à®®à®¿ à®‰à®°à¯à®£à¯à®Ÿà¯ˆà®¯à®¾à®©à®¤à¯ à®®à®±à¯à®±à¯à®®à¯ à®šà¯‚à®°à®¿à®¯à®©à¯ˆà®šà¯ à®šà¯à®±à¯à®±à®¿ à®µà®°à¯à®•à®¿à®±à®¤à¯.",
            "à²­à³‚à²®à²¿ à²—à³‹à²³à²¾à²•à²¾à²°à²¦à²²à³à²²à²¿à²¦à³† à²®à²¤à³à²¤à³ à²¸à³‚à²°à³à²¯à²¨ à²¸à³à²¤à³à²¤ à²¸à³à²¤à³à²¤à³à²¤à³à²¤à²¦à³†."
        ]
        
        # Combine and label data
        texts.extend(scam_texts)
        labels.extend([1] * len(scam_texts))  # 1 for scam
        
        texts.extend(legitimate_texts)
        labels.extend([0] * len(legitimate_texts))  # 0 for legitimate
        
        logger.info(f"Loaded {len(texts)} samples")
        return texts, labels
    
    def prepare_dataset(self, texts: List[str], labels: List[int]) -> Dataset:
        """Prepare dataset for training"""
        logger.info("Preparing dataset...")
        
        # Tokenize texts
        tokenized = self.tokenizer(
            texts,
            truncation=True,
            padding=True,
            max_length=512,
            return_tensors="pt"
        )
        
        # Create dataset
        dataset = Dataset.from_dict({
            "input_ids": tokenized["input_ids"],
            "attention_mask": tokenized["attention_mask"],
            "labels": labels
        })
        
        return dataset
    
    def compute_metrics(self, eval_pred):
        """Compute evaluation metrics"""
        predictions, labels = eval_pred
        predictions = np.argmax(predictions, axis=1)
        
        # Calculate accuracy
        accuracy = (predictions == labels).astype(np.float32).mean().item()
        
        return {"accuracy": accuracy}
    
    def train(self, data_path: str, output_dir: str = "models/classifier"):
        """Train the classifier"""
        logger.info("Starting training...")
        
        # Load data
        texts, labels = self.load_data(data_path)
        
        # Load tokenizer and model
        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
        self.model = AutoModelForSequenceClassification.from_pretrained(
            self.model_name,
            num_labels=2,
            problem_type="single_label_classification"
        )
        
        # Prepare dataset
        dataset = self.prepare_dataset(texts, labels)
        
        # Split dataset
        train_dataset, eval_dataset = train_test_split(
            dataset, 
            test_size=0.2, 
            random_state=42
        )
        
        # Training arguments
        self.training_args = TrainingArguments(
            output_dir=output_dir,
            num_train_epochs=3,
            per_device_train_batch_size=8,
            per_device_eval_batch_size=8,
            warmup_steps=100,
            weight_decay=0.01,
            logging_dir=f"{output_dir}/logs",
            logging_steps=10,
            evaluation_strategy="steps",
            eval_steps=50,
            save_strategy="steps",
            save_steps=100,
            load_best_model_at_end=True,
            metric_for_best_model="accuracy",
            greater_is_better=True,
        )
        
        # Data collator
        data_collator = DataCollatorWithPadding(tokenizer=self.tokenizer)
        
        # Create trainer
        trainer = Trainer(
            model=self.model,
            args=self.training_args,
            train_dataset=train_dataset,
            eval_dataset=eval_dataset,
            tokenizer=self.tokenizer,
            data_collator=data_collator,
            compute_metrics=self.compute_metrics,
        )
        
        # Train model
        logger.info("Training model...")
        trainer.train()
        
        # Save model
        logger.info(f"Saving model to {output_dir}")
        trainer.save_model()
        self.tokenizer.save_pretrained(output_dir)
        
        # Evaluate model
        logger.info("Evaluating model...")
        eval_results = trainer.evaluate()
        logger.info(f"Evaluation results: {eval_results}")
        
        # Save model metadata
        metadata = {
            "model_name": self.model_name,
            "num_labels": 2,
            "labels": ["legitimate", "scam"],
            "languages": ["en", "hi", "ta", "kn"],
            "thresholds": {
                "en": 0.92,
                "hi": 0.90,
                "ta": 0.90,
                "kn": 0.90
            },
            "evaluation_results": eval_results,
            "training_samples": len(texts)
        }
        
        with open(f"{output_dir}/model_metadata.json", "w") as f:
            json.dump(metadata, f, indent=2)
        
        logger.info("Training completed successfully!")
        return eval_results

def main():
    """Main training function"""
    trainer = ScamClassifierTrainer()
    
    # Create output directory
    os.makedirs("models/classifier", exist_ok=True)
    
    # Train model
    results = trainer.train("data/demo/sample_claims.json", "models/classifier")
    
    print("ðŸŽ‰ Training completed!")
    print(f"ðŸ“Š Final accuracy: {results['eval_accuracy']:.4f}")

if __name__ == "__main__":
    main()
